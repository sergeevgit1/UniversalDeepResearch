# Universal Deep Research Backend (UDR-B)

Backend-сервис на основе FastAPI, который предоставляет возможности интеллектуального исследования и создания отчётов с использованием больших языковых моделей и API веб-поиска. Система может выполнять всестороннее исследование по пользовательским запросам, агрегировать результаты и генерировать подробные отчёты.

Это программное обеспечение предоставляется исключительно в исследовательских и демонстрационных целях. Оно предназначено исключительно как прототип для демонстрации исследовательских концепций и методологий в области искусственного интеллекта и автоматизированных исследовательских систем.

- Это программное обеспечение не предназначено для производственного развёртывания, коммерческого использования или любого реального применения, где требуется надёжность, точность или безопасность.
- Это программное обеспечение содержит экспериментальные функции, непроверенные методологии и реализации исследовательского уровня, которые могут содержать ошибки, уязвимости безопасности или другие проблемы.
- Программное обеспечение предоставляется "КАК ЕСТЬ" без каких-либо гарантий. Ни NVIDIA Corporation, ни авторы не несут ответственности за любой ущерб, возникший в результате использования этого программного обеспечения, в максимальной степени, разрешённой законом.

Используя это программное обеспечение, вы подтверждаете, что прочитали и поняли полный файл DISCLAIMER и согласны соблюдать его условия. Для получения полного юридического отказа от ответственности, пожалуйста, ознакомьтесь с файлом [DISCLAIMER](../../apps/backend/DISCLAIMER.txt) в этом каталоге.

## Возможности

- **Интеллектуальное исследование**: Автоматизированный веб-поиск и анализ контента с использованием Tavily API
- **Поддержка множества моделей**: Настраиваемые LLM backend'ы (OpenAI, NVIDIA, локальный vLLM)
- **Потоковые ответы**: Обновления прогресса в реальном времени через Server-Sent Events
- **Управление сессиями**: Постоянные исследовательские сессии с уникальными идентификаторами
- **Гибкая архитектура**: Модульный дизайн с настраиваемыми компонентами
- **Режим пробного запуска**: Возможности тестирования с mock-данными
- **Расширенное фреймирование**: Пользовательская система FrameV4 для повышения надёжности следования инструкциям во всех моделях

## Архитектура

Backend состоит из нескольких ключевых компонентов:

- **`main.py`**: FastAPI-приложение с конечными точками исследований
- **`scan_research.py`**: Основная логика исследований и создания отчётов
- **`clients.py`**: Управление клиентами LLM и поисковых API
- **`frame/`**: Расширенный фреймворк надёжности (FrameV4)
- **`items.py`**: Утилиты для сохранения данных
- **`sessions.py`**: Генерация и управление ключами сессий

## Работа через Turborepo

- Монорепозиторий использует layout `apps/backend`, поэтому команды Turbo запускаются из корня.
- Типичные команды:
  - `turbo run dev --filter=apps/backend` — дев-сервер FastAPI (без кеша, с live reload).
  - `turbo run lint --filter=apps/backend` — быстрая проверка синтаксиса Python (`compileall`).
  - `turbo run build --filter=apps/backend` — подготовка артефактов для контейнеризации/деплоя.
- Окружение хранится в `apps/backend/.env` и не кешируется Turborepo; убедитесь, что файл создан перед запуском задач.
- Если кеш повредился, удалите `.turbo` и запустите команды снова (`TURBO_CACHE=none` для временного отключения кеша в CI).

## Быстрый старт

### Предварительные требования

- Python 3.8+
- API-ключи для выбранного вами провайдера LLM
- API-ключ Tavily для функциональности веб-поиска

### Установка

#### Вариант 1: Автоматическая установка (рекомендуется)

Самый простой способ настроить backend - использовать предоставленный скрипт `setup.py`:

1. **Клонируйте репозиторий**:

   ```bash
   git clone <repository-url>
   cd apps/backend
   ```

2. **Запустите скрипт установки**:

   ```bash
   python3 setup.py
   ```

   Скрипт установки выполнит:

   - Проверку совместимости версии Python
   - Создание необходимых директорий (`logs/`, `instances/`, `mock_instances/`)
   - Настройку конфигурации окружения (файл `.env`)
   - Проверку наличия необходимых файлов с API-ключами
   - Установку зависимостей Python
   - Проверку установки

3. **Настройте API-ключи**:
   Создайте следующие файлы с вашими API-ключами:

   ```bash
   echo "your-tavily-api-key" > tavily_api.txt
   echo "your-llm-api-key" > nvdev_api.txt  # или openai_api.txt
   ```

4. **Запустите сервер**:

   ```bash
   ./launch_server.sh
   ```

   **Примечание**: Скрипт `launch_server.sh` является рекомендуемым способом запуска сервера, так как он:

   - Автоматически загружает переменные окружения из `.env`
   - Устанавливает правильные конфигурации по умолчанию
   - Запускает сервер в фоновом режиме с логированием
   - Предоставляет информацию об управлении процессами

#### Вариант 2: Ручная установка

Если вы предпочитаете настроить backend вручную, выполните следующие шаги:

1. **Клонируйте репозиторий**:

   ```bash
   git clone <repository-url>
   cd apps/backend
   ```

2. **Создайте виртуальное окружение**:

   ```bash
   python3 -m venv venv
   source venv/bin/activate  # На Windows: venv\Scripts\activate
   ```

3. **Установите зависимости**:

   ```bash
   pip install -r requirements.txt
   ```

4. **Создайте необходимые директории**:

   ```bash
   mkdir -p logs instances mock_instances
   ```

5. **Настройте конфигурацию окружения**:
   Скопируйте пример файла окружения и настройте его:

   ```bash
   cp env.example .env
   # Отредактируйте файл .env с вашей конфигурацией
   ```

6. **Настройте API-ключи**:
   Создайте следующие файлы с вашими API-ключами:

   ```bash
   echo "your-tavily-api-key" > tavily_api.txt
   echo "your-llm-api-key" > nvdev_api.txt  # или, например, openai_api.txt
   ```

7. **Запустите сервер**:

   ```bash
   ./launch_server.sh
   ```

   **Примечание**: Как отмечено для варианта 1, скрипт `launch_server.sh` является рекомендуемым способом запуска сервера, так как он:

   - Автоматически загружает переменные окружения из `.env`
   - Устанавливает правильные конфигурации по умолчанию
   - Запускает сервер в фоновом режиме с логированием
   - Предоставляет информацию об управлении процессами

Сервер будет доступен по адресу `http://localhost:8000`

Теперь вы можете быстро протестировать сервер.

```bash
curl -X POST http://localhost:8000/api/research \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Какие последние разработки в области квантовых вычислений?",
    "start_from": "research"
  }'
```

## Конфигурация

### Переменные окружения

Создайте файл `.env` в директории `apps/backend`:

```env
# Конфигурация сервера
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=info

# Конфигурация CORS
FRONTEND_URL=http://localhost:3000

# Конфигурация модели
DEFAULT_MODEL=llama-3.1-nemotron-253b
LLM_BASE_URL=https://integrate.api.nvidia.com/v1
LLM_API_KEY_FILE=nvdev_api.txt

# Конфигурация поиска
TAVILY_API_KEY_FILE=tavily_api.txt

# Конфигурация исследований
MAX_TOPICS=1
MAX_SEARCH_PHRASES=1
MOCK_DIRECTORY=mock_instances/stocks_24th_3_sections

# Конфигурация логирования
LOG_DIR=logs
TRACE_ENABLED=true
```

### Конфигурация модели

Система поддерживает множество провайдеров LLM. Настройте модели в `clients.py`:

```python
MODEL_CONFIGS = {
    "llama-3.1-8b": {
        "base_url": "https://integrate.api.nvidia.com/v1",
        "api_type": "nvdev",
        "completion_config": {
            "model": "nvdev/meta/llama-3.1-8b-instruct",
            "temperature": 0.2,
            "top_p": 0.7,
            "max_tokens": 2048,
            "stream": True
        }
    },
    # Добавьте больше моделей по необходимости
}
```

### Файлы с API-ключами

Система ожидает API-ключи в текстовых файлах:

- `tavily_api.txt`: API-ключ поиска Tavily
- `nvdev_api.txt`: API-ключ NVIDIA
- `openai_api.txt`: API-ключ OpenAI

## Конечные точки API

### GET `/`

Конечная точка проверки работоспособности, которая возвращает сообщение о статусе.

### POST `/api/research`

Основная конечная точка исследований, которая выполняет исследование и генерирует отчёты.

**Тело запроса**:

```json
{
  "dry": false,
  "session_key": "optional-session-key",
  "start_from": "research",
  "prompt": "Ваш исследовательский запрос здесь",
  "mock_directory": "mock_instances/stocks_24th_3_sections"
}
```

**Параметры**:

- `dry` (boolean): Использовать mock-данные для тестирования
- `session_key` (string, необязательно): Существующая сессия для продолжения
- `start_from` (string): "research" или "reporting"
- `prompt` (string): Исследовательский запрос (обязателен для фазы исследования)
- `mock_directory` (string): Директория для mock-данных

**Ответ**: Поток Server-Sent Events с прогрессом исследования

### POST `/api/research2`

Конечная точка расширенного фреймворка надёжности с использованием системы FrameV4. Это конечная точка, которая поддерживает пользовательские стратегии глубокого исследования.

**Тело запроса**:

```json
{
  "prompt": "Ваш исследовательский запрос",
  "strategy_id": "custom-strategy",
  "strategy_content": "Пользовательская стратегия исследования"
}
```

## Примеры использования

### Базовый запрос исследования

```bash
curl -X POST http://localhost:8000/api/research \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Какие последние разработки в области квантовых вычислений?",
    "start_from": "research"
  }'
```

### Тестирование в режиме пробного запуска

```bash
curl -X POST http://localhost:8000/api/research \
  -H "Content-Type: application/json" \
  -d '{
    "dry": true,
    "prompt": "Тестовый исследовательский запрос",
    "start_from": "research"
  }'
```

### Продолжение с фазы создания отчёта

```bash
curl -X POST http://localhost:8000/api/research \
  -H "Content-Type: application/json" \
  -d '{
    "session_key": "20241201T120000Z-abc12345", # Это будет ключ сессии, которую вы ранее запустили
    "start_from": "reporting"
  }'
```

## Разработка

### Логирование

Логи хранятся в директории `logs/`:

- `comms_YYYYMMDD_HH-MM-SS.log`: Трассировки коммуникаций
- `{instance_id}_compilation.log`: Логи компиляции фреймов
- `{instance_id}_execution.log`: Логи выполнения фреймов

### Mock-данные

Mock-данные исследований доступны в `mock_instances/`:

- `stocks_24th_3_sections/`: Данные исследований фондового рынка
- `stocks_30th_short/`: Краткие данные фондового рынка

## Развёртывание

### Производственное развёртывание

1. **Настройте окружение**:

   ```bash
   export HOST=0.0.0.0
   export PORT=8000
   export LOG_LEVEL=info
   ```

2. **Запустите с gunicorn**:

   ```bash
   pip install gunicorn
   gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
   ```

   **Примечание**: Для разработки предпочтительнее использовать `./launch_server.sh`, который обеспечивает лучшее управление процессами и логирование.

### Развёртывание в Docker

Создайте `Dockerfile`:

```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

## Устранение неполадок

### Распространённые проблемы

1. **Ошибки API-ключей**: Убедитесь, что файлы с API-ключами существуют и содержат действительные ключи
2. **Ошибки CORS**: Проверьте конфигурацию `FRONTEND_URL`
3. **Ошибки модели**: Проверьте конфигурацию модели в `clients.py`
4. **Ошибки прав доступа**: Убедитесь в наличии прав на запись для директорий `logs/` и `instances/`

### Режим отладки

Включите отладочное логирование, установив переменную окружения LOG_LEVEL:

```bash
export LOG_LEVEL=debug
./launch_server.sh
```

Или запустите uvicorn напрямую для отладки:

```bash
uvicorn main:app --reload --log-level=debug
```

## Вклад в проект

1. Сделайте fork репозитория
2. Создайте ветку для функции
3. Внесите изменения
4. Добавьте тесты, если применимо
5. Отправьте pull request

## Лицензия и отказ от ответственности

Это программное обеспечение предоставляется только в исследовательских и демонстрационных целях. Пожалуйста, обратитесь к файлу [DISCLAIMER](DISCLAIMER.txt) для получения полных условий использования этого программного обеспечения. Вы можете найти лицензию в [LICENSE](LICENSE.txt).

**Не используйте этот код в production.**

## Поддержка

По вопросам и проблемам:

- Создайте issue в репозитории
- Проверьте логи в директории `logs/`
- Просмотрите настройки конфигурации
