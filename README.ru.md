# Universal Deep Research (UDR)

[English](README.md) | **Русский**

Прототип исследовательской системы, объединяющей пользовательские стратегии, интеллектуальный веб-поиск, анализ контента и автоматическую генерацию отчетов с использованием больших языковых моделей. Этот репозиторий содержит как backend API сервис, так и frontend веб-интерфейс.

Как упоминалось выше, это исследовательский демонстрационный прототип, который не следует использовать в производственных целях. Программное обеспечение содержит экспериментальные функции и реализации исследовательского уровня.

## Возможности

**Universal Deep Research** — это мощная система для автоматизации исследовательской работы, которая предоставляет следующие возможности:

- **Интеллектуальное исследование**: Настраиваемые пользователем стратегии исследования с использованием передовых LLM
- **Прогресс в реальном времени**: Живые обновления во время исследования и генерации отчетов через Server-Sent Events
- **Интерактивный интерфейс**: Современный веб-интерфейс для исследовательских запросов и результатов
- **Поддержка множества моделей**: Настраиваемые LLM бэкенды (NVIDIA, OpenAI, OpenRouter, локальный vLLM)
- **Веб-поиск**: Интеграция с Tavily API для получения актуальной информации из интернета
- **Автоматическая генерация отчетов**: Создание структурированных отчетов на основе собранных данных
- **Продвинутая надежность**: Система FrameV4 для повышения точности следования инструкциям
- **Гибкая архитектура**: Модульный дизайн с настраиваемыми компонентами

## Структура проекта

```
/
├── backend/          # FastAPI backend сервис
│   ├── README.md     # Документация backend
│   ├── main.py       # FastAPI приложение
│   ├── scan_research.py  # Основная логика исследования
│   ├── clients.py    # Управление LLM и поисковыми API клиентами
│   ├── config.py     # Конфигурация системы
│   ├── frame/        # Продвинутый фреймворк надежности
│   ├── Dockerfile    # Docker образ для backend
│   └── ...
├── frontend/         # Next.js frontend приложение
│   ├── README.md     # Документация frontend
│   ├── src/          # React компоненты и страницы
│   ├── Dockerfile    # Docker образ для frontend
│   └── ...
├── docker-compose.yml    # Оркестрация Docker сервисов
├── DOCKER.md            # Руководство по развертыванию Docker
├── QUICKSTART.md        # Быстрый старт
└── README.md            # Этот файл
```

## Быстрый старт

У вас есть два варианта запуска проекта: через Docker Compose (рекомендуется) или вручную.

### Вариант 1: Docker Compose (Рекомендуется)

Самый простой способ начать работу — использовать Docker Compose, который автоматически настроит оба сервиса. По умолчанию используется бесплатная модель OpenRouter.

```bash
# 1. Клонировать репозиторий
git clone https://github.com/sergeevgit1/UniversalDeepResearch.git
cd UniversalDeepResearch

# 2. Настроить API ключи
# OpenRouter теперь дефолтный провайдер
echo "ваш-openrouter-api-ключ" > backend/openrouter_api.txt
echo "ваш-tavily-api-ключ" > backend/tavily_api.txt

# Опционально: для NVIDIA
echo "ваш-nvidia-api-ключ" > backend/nvdev_api.txt

# 3. Запустить сервисы
docker compose up -d

# 4. Открыть приложение в браузере
# Frontend: http://localhost:3000
# Backend API: http://localhost:8000
```

Подробные инструкции по развертыванию Docker см. в [DOCKER.md](DOCKER.md) или [QUICKSTART.md](QUICKSTART.md).

### Вариант 2: Ручная установка

Для запуска прототипа вручную необходимо запустить оба сервиса — backend и frontend.

#### 1. Настройка Backend

Перейдите в директорию backend и следуйте инструкциям по установке:

```bash
cd backend
```

Подробные инструкции по настройке см. в [backend/README.md](backend/README.md), включая:

- Настройку окружения Python
- Конфигурацию API ключей
- Команды запуска сервера

#### 2. Настройка Frontend

В новом терминале перейдите в директорию frontend:

```bash
cd frontend
```

Подробные инструкции по настройке см. в [frontend/README.md](frontend/README.md), включая:

- Установку зависимостей Node.js
- Конфигурацию окружения
- Запуск сервера разработки

#### 3. Запуск прототипа

1. **Запустите backend сервер** (обычно на порту 8000):

   ```bash
   cd backend
   ./launch_server.sh
   ```

2. **Запустите frontend сервер разработки** (обычно на порту 3000):

   ```bash
   cd frontend
   npm run dev
   ```

3. **Откройте приложение**:
   Откройте браузер и перейдите по адресу `http://localhost:3000`.

## Поддерживаемые LLM провайдеры

Система поддерживает несколько провайдеров больших языковых моделей:

### NVIDIA NGC

NVIDIA предоставляет доступ к мощным моделям через API:

- **Llama 3.1 Nemotron 253B** (рекомендуется)
- **Llama 3.1 8B Instruct**
- И другие модели NVIDIA

Получите API ключ на [NVIDIA NGC](https://build.nvidia.com/).

### OpenAI

Поддержка моделей OpenAI:

- **GPT-4**
- **GPT-3.5 Turbo**
- И другие модели OpenAI

Получите API ключ на [OpenAI Platform](https://platform.openai.com/).

### OpenRouter

OpenRouter предоставляет унифицированный доступ к множеству LLM провайдеров через единый API:

- **Claude** (Anthropic)
- **GPT-4** (OpenAI)
- **Llama** (Meta)
- **Gemini** (Google)
- **Mistral** и многие другие

Преимущества OpenRouter:
- Единый API для доступа к десяткам моделей
- Конкурентные цены
- Автоматическое переключение между провайдерами
- Поддержка бесплатных моделей

Получите API ключ на [OpenRouter](https://openrouter.ai/).

### Локальный vLLM

Для приватности и контроля вы можете использовать локально развернутые модели через vLLM.

## Требования

### Для развертывания через Docker

- **Docker** 20.10+
- **Docker Compose** 2.0+
- **API ключи** для LLM провайдеров (NVIDIA NGC, OpenAI, или OpenRouter)
- **Tavily API ключ** для функциональности веб-поиска

### Для ручной установки

- **Python** 3.8+ (для backend)
- **Node.js** 18+ (для frontend)
- **API ключи** для LLM провайдеров (NVIDIA NGC, OpenAI, или OpenRouter)
- **Tavily API ключ** для функциональности веб-поиска

## Получение API ключей

### Tavily API (обязательно)

Tavily используется для веб-поиска и сбора информации:

1. Перейдите на [Tavily](https://tavily.com/)
2. Зарегистрируйтесь и получите API ключ
3. Сохраните ключ в файл `backend/tavily_api.txt`

### NVIDIA NGC API (опционально)

Для использования моделей NVIDIA:

1. Перейдите на [NVIDIA NGC](https://build.nvidia.com/)
2. Создайте аккаунт и получите API ключ
3. Сохраните ключ в файл `backend/nvdev_api.txt`

### OpenAI API (опционально)

Для использования моделей OpenAI:

1. Перейдите на [OpenAI Platform](https://platform.openai.com/)
2. Создайте аккаунт и получите API ключ
3. Сохраните ключ в файл `backend/openai_api.txt`

### OpenRouter API (опционально)

Для использования моделей через OpenRouter:

1. Перейдите на [OpenRouter](https://openrouter.ai/)
2. Создайте аккаунт и получите API ключ
3. Сохраните ключ в файл `backend/openrouter_api.txt`

## Конфигурация

### Переменные окружения

Основные настройки можно изменить через переменные окружения. Для Docker Compose создайте файл `.env`:

```bash
cp .env.example .env
```

Для ручной установки настройте файлы `.env` в директориях backend и frontend.

### Выбор модели

По умолчанию используется бесплатная модель OpenRouter: `openrouter/openai/gpt-oss-120b:free`. Вы можете изменить модель в конфигурации:

```env
# Дефолтная модель (бесплатная)
DEFAULT_MODEL=openrouter/openai/gpt-oss-120b-free

# Или использовать другую модель OpenRouter
DEFAULT_MODEL=openrouter/anthropic/claude-3.5-sonnet

# Или использовать NVIDIA
DEFAULT_MODEL=llama-3.1-nemotron-253b
```

## Архитектура

### Backend (FastAPI)

Backend построен на FastAPI и предоставляет RESTful API для исследований:

- **Streaming API**: Server-Sent Events для обновлений в реальном времени
- **Модульная архитектура**: Разделение на компоненты для легкой расширяемости
- **Управление сессиями**: Сохранение состояния исследований
- **FrameV4**: Продвинутая система для повышения надежности LLM

### Frontend (Next.js)

Frontend построен на Next.js 15 с React 19:

- **Server-Side Rendering**: Быстрая загрузка страниц
- **Real-time Updates**: WebSocket соединение для живых обновлений
- **Responsive Design**: Адаптивный интерфейс для всех устройств
- **TypeScript**: Типобезопасность и лучший DX

### Процесс исследования

1. **Пользовательский запрос**: Пользователь вводит исследовательский вопрос
2. **Генерация стратегии**: LLM создает план исследования
3. **Веб-поиск**: Tavily API ищет релевантную информацию
4. **Анализ контента**: LLM анализирует найденные материалы
5. **Генерация отчета**: Создается структурированный отчет
6. **Представление результатов**: Отчет отображается в веб-интерфейсе

## Документация

- [Руководство по развертыванию Docker](DOCKER.md) - Настройка и развертывание Docker Compose
- [Быстрый старт](QUICKSTART.md) - Краткая инструкция для быстрого запуска
- [Документация Backend](backend/README.md) - Настройка API, конфигурация и эндпоинты
- [Документация Frontend](frontend/README.md) - Настройка UI, конфигурация и развертывание

## Примеры использования

### Исследовательский запрос

```
Какие последние достижения в области квантовых вычислений в 2024 году?
```

Система:
1. Сгенерирует стратегию исследования
2. Выполнит веб-поиск по релевантным источникам
3. Проанализирует найденные статьи и публикации
4. Создаст структурированный отчет с выводами

### Пользовательская стратегия

Вы можете определить собственную стратегию исследования через API v2:

```json
{
  "prompt": "Анализ рынка электромобилей",
  "strategy_id": "market-analysis",
  "strategy_content": "1. Найти текущие тренды\n2. Сравнить ключевых игроков\n3. Проанализировать прогнозы"
}
```

## Разработка

### Локальная разработка

Для разработки рекомендуется запускать сервисы отдельно с hot-reload:

```bash
# Terminal 1 - Backend
cd backend
source venv/bin/activate
uvicorn main:app --reload

# Terminal 2 - Frontend
cd frontend
npm run dev
```

### Структура кода

- **Backend**: Модульная архитектура с разделением ответственности
- **Frontend**: Компонентный подход с переиспользуемыми элементами
- **Типизация**: TypeScript для frontend, type hints для Python

### Тестирование

```bash
# Backend тесты
cd backend
pytest

# Frontend тесты
cd frontend
npm test
```

## Управление Docker сервисами

### Просмотр логов

```bash
# Все сервисы
docker compose logs -f

# Только backend
docker compose logs -f backend

# Только frontend
docker compose logs -f frontend
```

### Перезапуск сервисов

```bash
# Перезапустить все
docker compose restart

# Перезапустить конкретный сервис
docker compose restart backend
```

### Остановка и удаление

```bash
# Остановить сервисы
docker compose down

# Остановить и удалить volumes
docker compose down -v
```

### Обновление после изменений

```bash
# Пересобрать и перезапустить
docker compose up -d --build
```

## Безопасность

**Важно**: Это исследовательский прототип и не должен использоваться в production без надлежащей защиты.

Рекомендации по безопасности:

- Не храните API ключи в коде или публичных репозиториях
- Используйте переменные окружения или секреты для чувствительных данных
- Настройте HTTPS для production развертывания
- Ограничьте CORS для production окружения
- Регулярно обновляйте зависимости
- Используйте Docker secrets для API ключей в production

## Производительность

### Оптимизация

- **Кэширование**: Backend кэширует результаты поиска
- **Streaming**: Результаты передаются по мере готовности
- **Параллелизм**: Множественные поисковые запросы выполняются параллельно
- **CDN**: Статические ресурсы frontend могут быть размещены на CDN

### Масштабирование

Для production развертывания рекомендуется:

- Использовать load balancer для распределения нагрузки
- Развернуть несколько инстансов backend
- Использовать Redis для кэширования и сессий
- Настроить автоскейлинг в облаке

## Устранение неполадок

### Backend не запускается

Проверьте:
- Наличие и корректность API ключей в файлах
- Установлены ли все зависимости Python
- Доступен ли порт 8000

### Frontend не подключается к Backend

Проверьте:
- Backend запущен и доступен
- Переменные окружения `NEXT_PUBLIC_BACKEND_*` настроены правильно
- CORS настроен для разрешения запросов от frontend

### Ошибки Docker

Проверьте:
- Docker daemon запущен
- Достаточно места на диске
- Файлы API ключей существуют и доступны для чтения

## Вклад в проект

Мы приветствуем вклад в развитие проекта! Пожалуйста:

1. Форкните репозиторий
2. Создайте feature ветку
3. Внесите изменения
4. Добавьте тесты при необходимости
5. Отправьте pull request

См. [CONTRIBUTING.md](backend/CONTRIBUTING.md) для деталей.

## Лицензия и отказ от ответственности

Это программное обеспечение предоставляется исключительно для исследовательских и демонстрационных целей. Пожалуйста, обратитесь к файлу [DISCLAIMER](backend/DISCLAIMER.txt) для полных условий использования программного обеспечения. Лицензию можно найти в [LICENSE](LICENSE.txt).

**Не используйте этот код в production.**

## Поддержка

Для вопросов и проблем:

- Создайте issue в репозитории
- Проверьте логи в директории `logs/`
- Просмотрите настройки конфигурации
- Обратитесь к документации

## Дополнительные ресурсы

- [NVIDIA NGC](https://build.nvidia.com/) - Платформа NVIDIA для AI
- [OpenRouter](https://openrouter.ai/) - Унифицированный доступ к LLM
- [Tavily](https://tavily.com/) - API для веб-поиска
- [FastAPI](https://fastapi.tiangolo.com/) - Документация FastAPI
- [Next.js](https://nextjs.org/) - Документация Next.js

## Roadmap

Планируемые улучшения:

- [ ] Поддержка дополнительных LLM провайдеров
- [ ] Улучшенное кэширование результатов
- [ ] Экспорт отчетов в различные форматы (PDF, DOCX)
- [ ] Аутентификация и авторизация пользователей
- [ ] Сохранение истории исследований
- [ ] Интеграция с базами данных для хранения результатов
- [ ] API для программного доступа
- [ ] Поддержка мультиязычности в интерфейсе

## Благодарности

Этот проект использует следующие открытые технологии:

- **FastAPI** - современный веб-фреймворк для Python
- **Next.js** - React фреймворк для production
- **OpenAI SDK** - для работы с LLM API
- **Tavily** - для веб-поиска
- **Docker** - для контейнеризации

---

**Примечание**: Это исследовательский прототип. Используйте на свой страх и риск и не применяйте в критически важных системах без соответствующей доработки и тестирования.
